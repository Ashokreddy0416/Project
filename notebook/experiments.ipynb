{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3915cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all_ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4e8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\LLMOPS\\Project\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c25af30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6e5f70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4258e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the question is asking for the capital of France. Hmm, let me think. I remember that France is a country in Europe, right? I think the capital is a city that starts with \"P.\" Wait, Paris? Yeah, that sounds familiar. I\\'ve heard of Paris being called the City of Light or something like that. But wait, is it definitely Paris? Maybe I\\'m mixing it up with another city. Let me check in my mind. Other major cities in France include Lyon, Marseille, Bordeaux, and maybe Toulouse. But the capital is almost certainly Paris. I think the French government is based there, with the Eiffel Tower and the Louvre. Yeah, that\\'s Paris. I don\\'t think there\\'s any other city that\\'s the capital. Wait, was there ever a different capital before? I don\\'t recall. I think it\\'s always been Paris. So yeah, the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is not only the political and administrative center of the country but also a major cultural, economic, and historical hub. Known as the \"City of Light\" (La Ville Lumière), Paris is famous for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. \\n\\n**Answer:** The capital of France is Paris.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5823d",
   "metadata": {},
   "source": [
    "1.Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0116678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a0cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c259af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc05b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\LLMOPS\\\\Project\\\\notebook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa8712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(os.getcwd(),\"data\",\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4037f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "971f3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68fcf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF2',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '',\n",
       " 'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       " 'publisher': 'Curran Associates, Inc.',\n",
       " 'language': 'en-US',\n",
       " 'created': '2017',\n",
       " 'eventtype': 'Poster',\n",
       " 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       " 'title': 'Attention is All you Need',\n",
       " 'date': '2017',\n",
       " 'moddate': '2018-02-12T21:22:10-08:00',\n",
       " 'published': '2017',\n",
       " 'type': 'Conference Proceedings',\n",
       " 'firstpage': '5998',\n",
       " 'book': 'Advances in Neural Information Processing Systems 30',\n",
       " 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       " 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       " 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       " 'lastpage': '6008',\n",
       " 'source': 'c:\\\\LLMOPS\\\\Project\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 11,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68440f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de643354",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1024,chunk_overlap=200,length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afb69f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c772068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4f49aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0f2cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cba9a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ff657d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 392.59it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.embed_query(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c93feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_query(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "922fd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=FAISS.from_documents(docs,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4813e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs=vector_store.similarity_search(\"what is self attention mechanism?\",k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a233615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say you don't know. {context} Question: {question} Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fa64c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b221250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b3508f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say you don't know. {context} Question: {question} Answer:\")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e50b7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4a4238b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cdcd3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3cefe0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b57c05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=((({\"context\":retriever|format_docs,\"question\":RunnablePassthrough()}\n",
    "   |prompt\n",
    "   |llm\n",
    "   |StrOutputParser())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "79969d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so the user is asking about the self-attention mechanism. Let me start by recalling what I read in the provided context. The first paragraph mentions that self-attention, also called intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation. It's used in tasks like reading comprehension and summarization.\\n\\nThen, there's a comparison with end-to-end memory networks that use recurrent attention. The Transformer is highlighted as the first model relying entirely on self-attention without RNNs or convolution. The key point is that self-attention allows the model to relate different positions in the sequence without sequential computation, which is a big deal because it enables parallel processing. \\n\\nThe context also talks about how in models like ConvS2S, the number of operations increases with the distance between positions, making distant dependencies harder to learn. The Transformer reduces this to a constant number of operations, though they mention an averaging effect that they counteract with Multi-Head Attention. \\n\\nAdditionally, self-attention might make models more interpretable since attention distributions can show what parts of the input the model focuses on. The training data part isn't directly relevant here, but the model architecture section describes the encoder-decoder structure with self-attention layers.\\n\\nPutting this together, the self-attention mechanism is a way for the model to weigh the importance of different parts of the input sequence relative to each other. It allows each position in the sequence to attend to all positions, including itself, which helps in capturing dependencies regardless of their distance. This is different from RNNs where information flows sequentially, making long-range dependencies harder. The Transformer's use of self-attention across all positions in parallel is what makes it efficient and effective for tasks needing understanding of context across the entire sequence.\\n</think>\\n\\nThe self-attention mechanism, also known as intra-attention, is an attention mechanism that relates different positions within a single sequence to compute its representation. It allows a model to dynamically weigh the importance of various parts of the input sequence relative to each other, enabling the capture of dependencies regardless of their distance. Key characteristics include:  \\n\\n1. **Parallel Computation**: Unlike recurrent models (e.g., RNNs) that process sequences sequentially, self-attention computes relationships between all positions simultaneously, reducing computational bottlenecks.  \\n2. **Position Independence**: The number of operations to relate any two positions is constant (unlike convolutional models where operations grow with distance), making it easier to learn long-range dependencies.  \\n3. **Interpretability**: Attention distributions reveal which parts of the input the model focuses on, with different attention heads often specializing in syntactic or semantic tasks.  \\n4. **Multi-Head Attention**: The Transformer uses multiple attention heads to mitigate the averaging effect of single-head attention, enhancing effective resolution.  \\n\\nSelf-attention is central to the Transformer architecture, replacing sequence-aligned recurrence or convolution entirely. It enables efficient, context-aware representations by allowing each position in the sequence to attend to all others, forming the foundation for tasks like translation, summarization, and language modeling.\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is self attention mechanism?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
