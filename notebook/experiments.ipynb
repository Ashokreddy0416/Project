{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3915cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all_ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4e8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\LLMOPS\\Project\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c25af30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5f70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4258e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out what the capital of France is. Let me start by recalling what I know about France. I remember that France is a country in Europe, and it\\'s known for cities like Paris, Lyon, Marseille, and Bordeaux. But which one is the capital? I think Paris might be the capital. Wait, but how sure am I? Let me think.\\n\\nI know that Paris is a major city in France, famous for the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. It\\'s a big tourist destination. Also, I\\'ve heard of Paris being referred to as the \"City of Light.\" But does that mean it\\'s the capital? Maybe. I also remember learning in school that the capital of France is Paris, but I need to confirm that. \\n\\nLet me consider other possibilities. Lyon is another large city; I think it was once the capital of the Roman Empire in the region, but that was a long time ago. I don\\'t think it\\'s the current capital. Marseille is a significant port city, but again, not the capital. Bordeaux is known for its wine, not likely to be the capital. \\n\\nI can also think about other countries and their capitals. For example, the capital of Germany is Berlin, Spain\\'s is Madrid, Italy\\'s is Rome. So France\\'s is Paris. That seems to fit the pattern. Also, in movies and books, when they mention France, they often refer to Paris as the main city. \\n\\nWait, but could there be confusion with the city of Paris being part of a larger region? Maybe Paris is part of √éle-de-France region, but the capital is still Paris itself. I think that\\'s right. \\n\\nAnother angle: government and political aspects. The French President\\'s office is in Paris, right? The √âlys√©e Palace is in Paris. The National Assembly is also there. So the government is centered in Paris, which would make it the capital. \\n\\nI might also check historical context. Has the capital ever been different? I think during the French Revolution, the capital was still Paris. Maybe during some periods it was moved, but not recently. \\n\\nSo putting it all together: Paris is the capital of France. I can\\'t think of any reason to doubt that. Unless there\\'s some recent change I\\'m not aware of, but I don\\'t recall hearing about France changing its capital. Therefore, the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\n**Key Points Supporting This Answer:**\\n1. **Geographical and Cultural Significance**: Paris is the largest city in France and a global hub for art, fashion, and culture, known for landmarks like the Eiffel Tower and the Louvre.\\n2. **Political Center**: The French government, including the President‚Äôs official residence (√âlys√©e Palace) and the National Assembly, is headquartered in Paris.\\n3. **Historical Context**: Paris has served as the capital of France for centuries, even during major historical events like the French Revolution.\\n4. **Regional Context**: While part of the √éle-de-France region, Paris itself is the capital city, not the region.\\n5. **Common Knowledge**: Paris is widely recognized as France\\'s capital in global references, education, and media.\\n\\n**Conclusion**: Paris remains the undisputed capital of France. üá´üá∑'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5823d",
   "metadata": {},
   "source": [
    "1.Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0116678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a0cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c259af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc05b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\LLMOPS\\\\Project\\\\notebook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa8712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(os.getcwd(),\"data\",\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4037f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "971f3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68fcf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF2',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '',\n",
       " 'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       " 'publisher': 'Curran Associates, Inc.',\n",
       " 'language': 'en-US',\n",
       " 'created': '2017',\n",
       " 'eventtype': 'Poster',\n",
       " 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       " 'title': 'Attention is All you Need',\n",
       " 'date': '2017',\n",
       " 'moddate': '2018-02-12T21:22:10-08:00',\n",
       " 'published': '2017',\n",
       " 'type': 'Conference Proceedings',\n",
       " 'firstpage': '5998',\n",
       " 'book': 'Advances in Neural Information Processing Systems 30',\n",
       " 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       " 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       " 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin',\n",
       " 'lastpage': '6008',\n",
       " 'source': 'c:\\\\LLMOPS\\\\Project\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 11,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68440f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de643354",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1024,chunk_overlap=200,length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afb69f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c772068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4f49aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefÔ¨Åcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0f2cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cba9a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ff657d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 392.59it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.embed_query(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c93feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_query(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "922fd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=FAISS.from_documents(docs,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4813e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs=vector_store.similarity_search(\"what is self attention mechanism?\",k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
